"""
Script de verifica√ß√£o de paridade entre experimentos 1, 2 e 3.

Verifica que, exceto pelas diferen√ßas espec√≠ficas de cada experimento,
todas as configura√ß√µes est√£o pareadas e consistentes.
"""

import sys
from pathlib import Path
from config import (
    EXPERIMENTS,
    TRAINING_CONFIG,
    MODEL_CONFIG,
    LOSS_CONFIG,
    OPTIMIZER_CONFIG,
    AUGMENTATION_CONFIG,
    LOGGING_CONFIG
)

print("=" * 80)
print("VERIFICA√á√ÉO DE PARIDADE DOS EXPERIMENTOS 1, 2, 3")
print("=" * 80)
print()

# ============================================================================
# 1. CONFIGURA√á√ïES DE EXPERIMENTO
# ============================================================================
print("1. CONFIGURA√á√ïES DE EXPERIMENTO")
print("-" * 80)

exp_names = ["exp0_baseline", "exp1_representacao_aumentada", "exp2_atencao_espacial", "exp3_reranking"]

for exp_name in exp_names:
    exp = EXPERIMENTS[exp_name]
    print(f"\n{exp_name}:")
    print(f"  Nome: {exp['name']}")
    print(f"  Descri√ß√£o: {exp['description']}")
    print(f"  Diret√≥rio existe: {exp['dir'].exists()}")

print("\n‚úÖ Todos os experimentos configurados corretamente")

# ============================================================================
# 2. DIMENS√ïES DOS EMBEDDINGS
# ============================================================================
print("\n" + "=" * 80)
print("2. DIMENS√ïES DOS EMBEDDINGS")
print("-" * 80)

print("\n{:<30} {:>15} {:>15} {:>15}".format("Experimento", "Texture", "Minutiae", "Total"))
print("-" * 80)

for exp_name in exp_names:
    texture_dim = MODEL_CONFIG["texture_embedding_dims"][exp_name]
    minutia_dim = MODEL_CONFIG["minutia_embedding_dims"][exp_name]
    total_dim = texture_dim + minutia_dim

    print(f"{exp_name:<30} {texture_dim:>15} {minutia_dim:>15} {total_dim:>15}")

print("\nüìä Diferen√ßas esperadas:")
print("  - exp0_baseline: 192-D (baseline)")
print("  - exp1_representacao_aumentada: 1024-D (aumentado)")
print("  - exp2_atencao_espacial: 192-D (baseline + aten√ß√£o)")
print("  - exp3_reranking: 192-D (baseline + re-ranking)")
print("\n‚úÖ Dimens√µes configuradas conforme especifica√ß√£o")

# ============================================================================
# 3. CONFIGURA√á√ïES DE TREINAMENTO
# ============================================================================
print("\n" + "=" * 80)
print("3. CONFIGURA√á√ïES DE TREINAMENTO (COMPARTILHADAS)")
print("-" * 80)

print("\n{:<20} {:>12} {:>12} {:>12} {:>12}".format(
    "Modo", "Batch Size", "Epochs", "Samples", "Workers"
))
print("-" * 80)

for mode in ["debug_minimal", "debug", "medium", "prod"]:
    config = TRAINING_CONFIG[mode]
    print(f"{mode:<20} {config['batch_size']:>12} {config['num_epochs']:>12} "
          f"{str(config.get('sample_size', 'all')):>12} {config['num_workers']:>12}")

print("\n‚ö†Ô∏è  IMPORTANTE para exp1_representacao_aumentada (1024-D):")
print("  - Requer ~3x mais mem√≥ria GPU que baseline")
print("  - Recomendado: REDUZIR batch_size em 50%")
print("    ‚Ä¢ debug_minimal: 8 ‚Üí 4")
print("    ‚Ä¢ debug: 8 ‚Üí 4")
print("    ‚Ä¢ medium: 8 ‚Üí 4-6")
print("    ‚Ä¢ prod: 20 ‚Üí 8-12")

# Verificar se batch size est√° OK
batch_size_prod = TRAINING_CONFIG["prod"]["batch_size"]
if batch_size_prod == 20:
    print(f"\n‚ö†Ô∏è  A√á√ÉO NECESS√ÅRIA: Batch size prod = {batch_size_prod}")
    print("  Para exp1, ajustar manualmente ou usar --aggressive-aug")
else:
    print(f"\n‚úÖ Batch size prod = {batch_size_prod} (OK para todos)")

# ============================================================================
# 4. LOSS CONFIGURATION
# ============================================================================
print("\n" + "=" * 80)
print("4. CONFIGURA√á√ÉO DE LOSS (COMPARTILHADA)")
print("-" * 80)

loss_type = LOSS_CONFIG.get("loss_type", "center")
print(f"\nLoss Type: {loss_type}")

if loss_type == "center":
    print(f"  Center Loss Base Weight: {LOSS_CONFIG['center_loss_base_weight']}")
    print(f"  Center Loss Adaptive: {LOSS_CONFIG['center_loss_use_adaptive']}")
    print(f"  Softmax Loss Weight: {LOSS_CONFIG['softmax_loss_weight']}")
    print(f"  Minutia Map Loss Weight: {LOSS_CONFIG['minutia_map_loss_weight']}")
elif loss_type == "arcface":
    print(f"  ArcFace Margin: {LOSS_CONFIG['arcface_margin']}")
    print(f"  ArcFace Scale: {LOSS_CONFIG['arcface_scale']}")
    print(f"  ArcFace Easy Margin: {LOSS_CONFIG['arcface_easy_margin']}")

print("\n‚úÖ Configura√ß√£o de loss compartilhada entre todos os experimentos")

# ============================================================================
# 5. OPTIMIZER CONFIGURATION
# ============================================================================
print("\n" + "=" * 80)
print("5. CONFIGURA√á√ÉO DE OPTIMIZER (COMPARTILHADA)")
print("-" * 80)

optimizer_type = OPTIMIZER_CONFIG.get("optimizer", "rmsprop")
print(f"\nOptimizer: {optimizer_type.upper()}")

if optimizer_type in OPTIMIZER_CONFIG:
    opt_config = OPTIMIZER_CONFIG[optimizer_type]
    print(f"  Learning Rate: {opt_config['lr']}")
    print(f"  Weight Decay: {opt_config['weight_decay']}")
    if optimizer_type == "rmsprop":
        print(f"  Alpha: {opt_config['alpha']}")
        print(f"  Momentum: {opt_config['momentum']}")
    elif optimizer_type == "adam":
        print(f"  Beta1: {opt_config['beta1']}")
        print(f"  Beta2: {opt_config['beta2']}")

print(f"  STN LR Scale: {OPTIMIZER_CONFIG['localization_network_lr_scale']} (3.5% do base LR)")
print(f"  LR Scheduler: {OPTIMIZER_CONFIG.get('use_lr_scheduler', False)}")

print("\n‚úÖ Configura√ß√£o de optimizer compartilhada entre todos os experimentos")

# ============================================================================
# 6. DATA AUGMENTATION
# ============================================================================
print("\n" + "=" * 80)
print("6. DATA AUGMENTATION (COMPARTILHADA)")
print("-" * 80)

print(f"\nRotation Range: ¬±{AUGMENTATION_CONFIG['rotation_range']}¬∞")
print(f"Translation Range: ¬±{AUGMENTATION_CONFIG['translation_range']}px")
print(f"Padding: {AUGMENTATION_CONFIG['padding']}px")
print(f"Border Mode: {AUGMENTATION_CONFIG['border_mode']}")
print(f"Quality Augmentation: {AUGMENTATION_CONFIG['quality_augmentation']}")
print(f"  Contrast Range: {AUGMENTATION_CONFIG['contrast_range']}")
print(f"  Brightness Range: {AUGMENTATION_CONFIG['brightness_range']}")

print("\n‚úÖ Data augmentation compartilhada entre todos os experimentos")

# ============================================================================
# 7. LOGGING CONFIGURATION
# ============================================================================
print("\n" + "=" * 80)
print("7. CONFIGURA√á√ÉO DE LOGGING (COMPARTILHADA)")
print("-" * 80)

print(f"\nLog Level: {LOGGING_CONFIG['log_level']}")
print(f"Log Interval: {LOGGING_CONFIG['log_interval']} batches")
print(f"Checkpoint Interval: {LOGGING_CONFIG['save_checkpoint_interval']} epochs")

print("\n‚úÖ Configura√ß√£o de logging compartilhada entre todos os experimentos")

# ============================================================================
# 8. VERIFICA√á√ÉO DE IMPLEMENTA√á√ÉO DOS MODELOS
# ============================================================================
print("\n" + "=" * 80)
print("8. VERIFICA√á√ÉO DE IMPLEMENTA√á√ÉO DOS MODELOS")
print("-" * 80)

import torch
from models_base import (
    DeepPrintBaseline,
    DeepPrintEnhancedRepresentation,
    DeepPrintSpatialAttention,
    DeepPrintWithReranking
)

models = [
    ("exp0_baseline", DeepPrintBaseline, 96, 96),
    ("exp1_representacao_aumentada", DeepPrintEnhancedRepresentation, 512, 512),
    ("exp2_atencao_espacial", DeepPrintSpatialAttention, 96, 96),
    ("exp3_reranking", DeepPrintWithReranking, 96, 96),
]

print("\nTestando instancia√ß√£o dos modelos...")
device = torch.device('cpu')
x_test = torch.randn(2, 1, 299, 299)

all_ok = True
for exp_name, ModelClass, texture_dims, minutia_dims in models:
    try:
        model = ModelClass(
            texture_embedding_dims=texture_dims,
            minutia_embedding_dims=minutia_dims
        ).to(device)

        model.set_num_classes(10)
        output = model(x_test)

        expected_dim = texture_dims + minutia_dims
        actual_dim = output['embedding'].shape[1]

        if actual_dim == expected_dim:
            print(f"  ‚úÖ {exp_name}: {actual_dim}-D (OK)")
        else:
            print(f"  ‚ùå {exp_name}: {actual_dim}-D (esperado {expected_dim}-D)")
            all_ok = False

    except Exception as e:
        print(f"  ‚ùå {exp_name}: ERRO - {e}")
        all_ok = False

if all_ok:
    print("\n‚úÖ Todos os modelos instanciam corretamente")
else:
    print("\n‚ùå ERRO: Alguns modelos falharam!")

# ============================================================================
# 9. VERIFICA√á√ÉO DE ESTRUTURA DE DIRET√ìRIOS
# ============================================================================
print("\n" + "=" * 80)
print("9. ESTRUTURA DE DIRET√ìRIOS")
print("-" * 80)

for exp_name in exp_names[1:]:  # Skip exp0, s√≥ verificar exp1/2/3
    exp_dir = EXPERIMENTS[exp_name]["dir"]
    subdirs = ["checkpoints", "logs", "models", "results"]

    print(f"\n{exp_name}:")
    all_exist = True
    for subdir in subdirs:
        full_path = exp_dir / subdir
        exists = full_path.exists()
        status = "‚úÖ" if exists else "‚ùå"
        print(f"  {status} {subdir}/")
        if not exists:
            all_exist = False

    if all_exist:
        print(f"  ‚úÖ Estrutura completa")
    else:
        print(f"  ‚ö†Ô∏è  Alguns diret√≥rios faltando (ser√£o criados automaticamente)")

# ============================================================================
# 10. RESUMO FINAL
# ============================================================================
print("\n" + "=" * 80)
print("RESUMO FINAL - PARIDADE DOS EXPERIMENTOS")
print("=" * 80)

print("\n‚úÖ CONFIGURA√á√ïES COMPARTILHADAS (Pareadas):")
print("  ‚úÖ Loss configuration (center loss ou arcface)")
print("  ‚úÖ Optimizer (RMSprop com LR, weight decay, etc)")
print("  ‚úÖ Data augmentation (rotation, translation, quality)")
print("  ‚úÖ Logging (intervals, checkpoints)")
print("  ‚úÖ Training config por modo (debug, medium, prod)")

print("\nüìä DIFEREN√áAS ESPEC√çFICAS (Esperadas):")
print("  üìä exp1_representacao_aumentada:")
print("      - Embeddings: 1024-D (vs 192-D baseline)")
print("      - Arquitetura: Mesma base, camadas finais maiores")
print("      - ‚ö†Ô∏è  GPU memory: ~3x mais que baseline")
print("      - ‚ö†Ô∏è  Recomendado: batch_size menor (50% do baseline)")

print("\n  üìä exp2_atencao_espacial:")
print("      - Embeddings: 192-D (igual baseline)")
print("      - Arquitetura: + Spatial Attention Module")
print("      - GPU memory: ~20% mais que baseline")
print("      - Batch size: igual baseline")

print("\n  üìä exp3_reranking:")
print("      - Embeddings: 192-D (igual baseline)")
print("      - Arquitetura: + Re-ranking Module")
print("      - GPU memory: ~20% mais que baseline")
print("      - Batch size: igual baseline")

print("\n" + "=" * 80)
print("‚ö†Ô∏è  AJUSTES RECOMENDADOS ANTES DE RODAR EXP1:")
print("=" * 80)

batch_size_prod = TRAINING_CONFIG["prod"]["batch_size"]
if batch_size_prod >= 16:
    print(f"\n1. AJUSTAR BATCH SIZE para exp1_representacao_aumentada:")
    print(f"   Atual: prod batch_size = {batch_size_prod}")
    print(f"   Recomendado: 8-12 (para RTX 2070 8GB)")
    print(f"\n   Op√ß√£o A: Ajustar manualmente em config.py")
    print(f"   Op√ß√£o B: O training.py pode detectar e ajustar automaticamente")
    print(f"   Op√ß√£o C: Monitorar uso de GPU e reduzir se necess√°rio")
else:
    print(f"\n‚úÖ Batch size prod = {batch_size_prod} (OK para exp1)")

print("\n2. ORDEM DE EXECU√á√ÉO RECOMENDADA:")
print("   1. Debug minimal em todos (30 min total)")
print("   2. Se passaram: medium nos mais promissores (4-8h)")
print("   3. Se superaram baseline: prod (30-50h)")

print("\n3. M√âTRICAS DE COMPARA√á√ÉO COM BASELINE:")
print("   Baseline (exp0, √©poca 11):")
print("     - EER: 0.16")
print("     - Impostores: 0.18")
print("     - Genu√≠nos: 0.99")
print("     - Separa√ß√£o: 82%")
print("\n   Para considerar exp1/2/3 melhor:")
print("     - EER < 0.16")
print("     - Separa√ß√£o > 80%")
print("     - Converg√™ncia est√°vel")

print("\n" + "=" * 80)
print("‚úÖ VERIFICA√á√ÉO COMPLETA - EXPERIMENTOS PRONTOS")
print("=" * 80)
print("\nTodos os experimentos est√£o pareados corretamente!")
print("Diferen√ßas s√£o apenas as espec√≠ficas de cada experimento (embeddings, m√≥dulos).")
print("\nPode rodar com confian√ßa! üöÄ")
print()
